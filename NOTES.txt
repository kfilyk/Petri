NOTES

1.18:
11/4/17: Thinking about testing a new learning strategy.
  1: Given l layers in network, layers 1 to (l-1) are initialized such that they output the input vector after each layer, exactly.
  Over several iterations, unique weights/biases mutate in the uniform layers, allowing complex decision
  paths to grow as the number of variations between uniform layers increase.
  2: Keep track of the stability of certain neurons, by analyzing how flexible the rates of change are for their weights and biases.
  Suppose a child creature of some parent dies. If some neuron gene G in the parent has a high Rate Of Mutation,
  assume that the ROM is linked to the poor outcome of the child, since one of its weight values has been changed.
  Then it makes sense (?) to reduce the ROM of G in the parent -> G-related weight value in future children remains the same.
  3: Keep track of the variability of each gene over the scope of the lineage.
  4: ...


  HOW TO GET SAME INPUT AFTER EACH LAYER:
  Suppose input = 0.7. Goes into neuron in first layer.
  ....

  Input transfer from ONE neuron NI to ONE adjacent NA:

  NI*W -> (FRACTION OF )


  SUM(INPUT/42)


  Only the last layer (before output) is randomly initialized (using box-muller normal dist).
  ....



1.15:
5/4/17: IS REGRESS BEING DONE WRONG? Currently, I am finding difference between ANY ANCESTOR and the unsuccessful descendant,
  then subtracting that difference from the gene pool. Instead, try finding difference between unsuccessful descendant and the
  PARENT of that descendant, then subtracting THAT instead... Meanwhile, propagate function works rediculously TOO well...

1.08:
23/3/17: Use 3 or more score types: lifespan, energy gained, number of children.

1.07:
22/3/17: Possibly move range into float values??

1.08:
22/3/17: Switching to multiplication. Keep biases, w1, w2. Use negative values?
1.12:
31/3/17: Switched to multiplication. Lots of changes. Using abs function generally to compute values. Using box-muller distribution of
  initial weights and bias values (normal dist. centered at 0).
  Going to implement alpha values for all neurons such that the shape of the abs function is more flexible.
  All in all, simulation is more viable than ever. Initial pop has a high survivability rate, I've managed to cut back on numerous
  variables in an effort to simply properties, graphs have been put into use (super helpful), and ive learned about and switched to
  using prototype functions for all objects (+3FPS, WAY better memory usage). Hopefully only one or two final changes to be made.
4/4/17: Changed my mind about using alpha value. Unneccesary added complexity. Since softsign approximates sigmoid, just use a
  corresponding rational of weights/bias == plenty of flexibility.

/*
0.29:
28/2/17: Clamped values from outputs 3 & 2 (eating col/type, respectively)
29/2/17: Idea: Allow mouth(s), eyes to detect similarity in names of other animals:
	(AAAA <-> AAAA) = 1
	(AAAA <-> AAAB) = 0.8
	(AAAA <-> AABB) = 0.4
	(AAAA <-> ABBB) = -0.2
	(AAAA <-> BBBB) = -1
	(AAAA <-> BBBA) = -0.8
	(AAAA <-> BBAA) = -0.4
	(AAAA <-> BAAA) = 0.2
	...
	Letters in a name have influence of 0.4, 0.3, 0.2, 0.1 respectively.
	It will take eyeNumber+mouthNumber (currently 6) neurons to implement... maybe wait a while...

  for (var i = 0; i < someObject.anArray.length ; i++) {
      if (someObject.anArray[i].point.x > someObject.anArray.point.y) {
         someFunction( someObject.anArray[i].point );
      }
  }

  var pointArray = someObject.anArray, thisPoint=null;
  for (var i = 0; i < pointArray.length ; i++) {
      thisPoint = anArray[i];
      if ( thisPoint.x > thisPoint.y ) {
         someFunction( thisPoint) ;
      }
  }

*/


0.29:
29/2/17: Best score = how long animal has been alive for... Possibly implement score which stacks when parent dies (this.score = par.score+this.score)? Growth rate could get out of hand...

0.29:
29/2/17: Implementing gradient descent via a "back-propagation" is possible- not going to be scientifically
accurate definition, just an experimental attempt at creating a recurrent mutation tweaker that follows a similar idea.
Have to be careful that I do not influence evolution by accidentally creating bias for creatures with high longevity...
TBC in 0.30:
1. For every mutable trait, establish a "learning rate" LR that is small in proportion to the trait.
	Example: For minSize, a good learning rate would be +-0.1, such that each mutant minSize value increments by that amount.
2. Initial (random) creatures will be created with an LR array of size n=number of mutable traits.
	Initial LR values will be set to a +- "unit value" for that trait (+-0.1 for minSize, +-1 for color, etc...).
	Children are born with identical LRs to the parent (for now).
3. Every child will contribute to the parents mutation rates, such that the death/success of each child pushes/pulls
	on the LRs of the parent. When a child dies, the LR of the parent changes incrementally in the direction
	opposite to the trait difference between the parent and child. Example: Suppose a child C has a minSize of 7.5 (~8) and
	the parent P has a minSize of 7.2 (~7). Suppose the LR of the parent is 0.3. If C dies, C causes the LR of P to
	decrease by 0.1. Then the failure of C has influenced all new children of P, such that the minSize grows less quickly
	for all future children of P (will not approach 8). The LR of P for minSize is now 0.2 (decreases by 0.1, the unit value).
	Then children born to P will have a minSize of 7.4 (~7) and a LR of 0.2 (same as the parent).
4. For now, only children deaths influence parent LRs. Parents have no influence on children LRs upon death. Success rate/
	highscore has no influence on LR either. Parent LRs only change in response to child deaths when the parent is ALIVE.
	"Success" then becomes implicative- the creatures who reproduce will be the ones who succeed, because their
	lineages will be the only ones around (Unless they all randomly start off craving the sweet cradle of death).
	This is what I am HOPING FOR- i've seen this somewhat in previous versions, but no way of telling what will happen next .
	Hypothetically, success should steer in the direction of longevity, and backprop will steer in the direction of
	self-perfecting form- if a petri is seldom to reproduce, then it has no offspring capable of directing the
	survivability of further offspring -> lineage dies. Inevitably, parent dies, due to changing environment/
	success of other lineages/being eaten. Remaining population consists of petris/lineages who adapted...

/*
0.28
22/02/17: Created mutable "sens" values: sensitivity values for used for neural responsiveness to input stimulus
27/02/17: Need recoil vals for vel + rot?
*/

/*
0.24
11/2/17: Okay, so if you keep yourself alive a long time, thats awesome. You get points for a long lifespan.
You ALSO gain points if you eat a lot of food. Makes sense. use echange/age??
0.25
13/2/17: Nope. Tried using age score, incrementing every frame, added +1 if foodEnergy>0 per frame. Pretty good,
  but after the cycle bloom success of intelligent eating habits loses out to straight herbivore (too much food).
  Going to try to counter balance this with -1 if foodEnergy<0.
13/2/17: RESOUNDING SUCCESS! Score inc. per frame +-1 if foodEnergy<>0. Generates *intelligent* population
  by gen ~25.
13/2/17: Cell damage rate is a ratio of (foodEnergyloss)/(minSize energy). In 0.26, test implementation
  of foodenergy/(size energy)- better imitation of nature, where large creatures possess incrementally
  more superior DNA repair mechanisms/longer lifespans. Reducing cell damage for larger size also takes
  away from survival stress of large creatures in simulation, and nudges in that direction.
16/3/17: Scrap all that. Now using backpropogation to control mutation rates/gradient descent of attributes.
21/3/17: If a child does not attain a number of children greater or equal to the parents,
  where child.score<=child.cno (cno==index of child in parents' child array), then backpropogation affects parent.
*/

0.23:
11/2/17: WTF is the best way to balance score?? rn its foodEnergy/age but it could still change

/*
0.20:
5/2/17: Tried tiles with regen rate of 0.2 instead of 0.1 = animals with less eyes survive/propagate more easily initially...?
5/2/17: No, i guess not..
6/2/17: Tile Col Range:
  R= [-30,150]
  G= [-30,200]
  B= [-30,100]
6/2/17: No more math.random() call every time "regenerate" is called. Replaced with standard regenRate for every tile.

*/

/*
0.20:
5/2/17: Minimized the range of the bias, weights1, and weights2: allows for more flexibility in neuron decision mutation, strengthens over generations.
  Bias vals can go as low as "round((Math.random()*10)-5)/100" without problems; weights vals "round((Math.random()*40)-20)/100".
0.22:
10/2/17: Discovered 32 bit float vals= +3 FPS. however my recent choice to draw all creature mouths+eyes oncanvas = -3 FPS. Oh well.
0.23:
11/2/17: Increased weights, decreased bias -> greater variation/range in output signals -> greater adaptability/flexible reasoning potential? But still keep it low as possible.
11/2/17: No, thats so wrong. Biases are as flexible as weights.
19/2/17: OutSignal has been modded to be 2* softsign, such that when x==1, y==1. Just trying it.
19/2/17: That was interesting... but terrible. Way too oversensitive. Back to softsign... for now.
19/2/17: JK... trying 2*(x^3)/(x^2)....
19/2/17: ... That might be way better... too early to tell. Previous softsign did not remotely allow animals to access highest settings of output functions... new activation function changes that at least.
0.25:
20/2/17: Another possibility:((2x)^3)/(1+(3x)^2) = 8(x^3)/(1+9(x^2))

  this.outSignal=function(ni) {
    var x=(this.is+this.bias[ni]);
    return x/(100+(x>=0 ? x:-x));
  }
  this.outSignal=function(ni) {
    var x=((this.is+this.bias[ni])/100);
    return 2*((1/(1+Math.exp(-x)))-0.5);
  }
  this.outSignal=function(ni) {
    var x=((this.is+this.bias[ni])/100);
    return (8*x*x*x)/(1+(9*x*x));
  }
7/3/17: In order to speed up mutations (while also not causing chaos), create variable neumut, with range [0.0 -> 1.0] (increments in tenths).
  this variable determines how much mutrate values change per generation (+-neumut), and is directed via backprop
17/3/17: Neumut sucks. Need to use more accurate method to predict descent.
17/3/17: No more I can do on Javascript; too slow- need to proceed from using linear values to multiplicative or softsign function, if possible.
  Time to humble myself and write in c++. Also, If initial input values are positive/negative/extreme,
  shouldnt we simply manipulate the init inputs at each layer, rather than generating new numbers at each layer? Should switch to a
  multiclass function instead of random int values.
*/

/*
0.20:
5/2/17: Tile Col Range:
  R= [-30,150]
  G= [-30,200]
  B= [-30,100]
  Such that for R, -60 centers on average, range between [-30, 150] (180 total)
*/

/*
0.20:
5/2/17: Color Types: 0=red, 1=green, 2=blue
28/2/17: Score based on success of lineage
*/

/*
0.20:
7/2/17: Difficulty to obtain food decreased; animals dont lose huge energy if eating wrong color. Otherwise no diversity/mutation/happiness
10/2/17: Future Goals: 1. Make them hate each other 2. Make them eat each other
20/2/17: No food if not carnivore? Probs makes sense. No free rides. However, try to help animals with carnivorous
habits- chasing, etc- dont make it brutal for carnivores to survive- they will be the smartest creatures.
*/
